{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Time Series Prediction with Wheat Prices\n\nThis is a short time series example with the Kaggle Dataset [Cerial Prices Changes Within Last 30 Years](https://www.kaggle.com/datasets/timmofeyy/-cerial-prices-changes-within-last-30-years?select=rice_wheat_corn_prices.csv).\n\n## Introduction to the Data\n\nAs described in **Kaggle**, the variables are as follows:\n\n* **Year :** A year starting from 1992 ending with 2022\n* **Month :** A month of the year\n* **Price_wheat_ton :** Wheat price per ton in the definite month and year\n* **Price_wheat_ton_infl :** Modern price per ton of wheat\n* **Price_rice_ton :** Price price per ton in the definite month and year\n* **Price_rice_ton_infl :** Modern rice per ton of rice\n* **Price_corn_ton** : Corn price per ton in the definite month and year\n* **Price_corn_ton_inf :** Modern price per ton of corn\n* **Inflation_rate :** Prices are generally continuous. For example this year price is a 1 dollar for a corn, but next year maybe 2 dollar.\n\nHowever, we will only use the variables **Year**, **Month**, and **Price_wheat_ton** solely for this project.\n\n## Our Goals and Analysis for this Data\n\nSo since we're only analyzing the wheat price per ton for that specific month and year, we wanted to see the trend for the price of wheat, compare the predictability of different time series models that we're going to create, and the pros and cons of each model. Specifically, our goals are:\n\n* Analyze the pricing trend and determine any seasonal effects \n* Use prior years' price to predict the price of wheat using a series of models (linear regression models, splines, and neural networks, etc) for **one year- 2021**.\n* Analyze the pros anc cons of each model in predicting 2021 price data.\n\n\n**We will now setup and load the packages we need along with the data.**","metadata":{}},{"cell_type":"code","source":"import os\nimport datetime\n\nimport IPython\nimport IPython.display\nimport matplotlib as mpl\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport pandas as pd\nimport seaborn as sns\nimport tensorflow as tf\n\nfrom scipy.signal import periodogram\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.model_selection import train_test_split\nfrom statsmodels.graphics.tsaplots import plot_pacf\n\n\n# Set Matplotlib defaults\nplt.style.use(\"seaborn-whitegrid\")\nplt.rc(\"figure\", autolayout=True, figsize=(11, 4))\nplt.rc(\n    \"axes\",\n    labelweight=\"bold\",\n    labelsize=\"large\",\n    titleweight=\"bold\",\n    titlesize=16,\n    titlepad=10,\n)\nplot_params = dict(\n    color=\"0.75\",\n    style=\".-\",\n    markeredgecolor=\"0.25\",\n    markerfacecolor=\"0.25\",\n)\n%config InlineBackend.figure_format = 'retina'\n\n\ndef lagplot(x, y=None, lag=1, standardize=False, ax=None, **kwargs):\n    from matplotlib.offsetbox import AnchoredText\n    x_ = x.shift(lag)\n    if standardize:\n        x_ = (x_ - x_.mean()) / x_.std()\n    if y is not None:\n        y_ = (y - y.mean()) / y.std() if standardize else y\n    else:\n        y_ = x\n    corr = y_.corr(x_)\n    if ax is None:\n        fig, ax = plt.subplots()\n    scatter_kws = dict(\n        alpha=0.75,\n        s=3,\n    )\n    line_kws = dict(color='C3', )\n    ax = sns.regplot(x=x_,\n                     y=y_,\n                     scatter_kws=scatter_kws,\n                     line_kws=line_kws,\n                     lowess=True,\n                     ax=ax,\n                     **kwargs)\n    at = AnchoredText(\n        f\"{corr:.2f}\",\n        prop=dict(size=\"large\"),\n        frameon=True,\n        loc=\"upper left\",\n    )\n    at.patch.set_boxstyle(\"square, pad=0.0\")\n    ax.add_artist(at)\n    ax.set(title=f\"Lag {lag}\", xlabel=x_.name, ylabel=y_.name)\n    return ax\n\n\ndef plot_lags(x, y=None, lags=6, nrows=1, lagplot_kwargs={}, **kwargs):\n    import math\n    kwargs.setdefault('nrows', nrows)\n    kwargs.setdefault('ncols', math.ceil(lags / nrows))\n    kwargs.setdefault('figsize', (kwargs['ncols'] * 2, nrows * 2 + 0.5))\n    fig, axs = plt.subplots(sharex=True, sharey=True, squeeze=False, **kwargs)\n    for ax, k in zip(fig.get_axes(), range(kwargs['nrows'] * kwargs['ncols'])):\n        if k + 1 <= lags:\n            ax = lagplot(x, y, lag=k + 1, ax=ax, **lagplot_kwargs)\n            ax.set_title(f\"Lag {k + 1}\", fontdict=dict(fontsize=14))\n            ax.set(xlabel=\"\", ylabel=\"\")\n        else:\n            ax.axis('off')\n    plt.setp(axs[-1, :], xlabel=x.name)\n    plt.setp(axs[:, 0], ylabel=y.name if y is not None else x.name)\n    fig.tight_layout(w_pad=0.1, h_pad=0.1)\n    return fig\n","metadata":{"execution":{"iopub.status.busy":"2022-03-26T21:09:39.427756Z","iopub.execute_input":"2022-03-26T21:09:39.428226Z","iopub.status.idle":"2022-03-26T21:09:47.223430Z","shell.execute_reply.started":"2022-03-26T21:09:39.428113Z","shell.execute_reply":"2022-03-26T21:09:47.222458Z"},"trusted":true},"execution_count":1,"outputs":[]},{"cell_type":"code","source":"df = pd.read_csv('../input/-cerial-prices-changes-within-last-30-years/rice_wheat_corn_prices.csv')\ndf","metadata":{"execution":{"iopub.status.busy":"2022-03-26T21:09:47.225366Z","iopub.execute_input":"2022-03-26T21:09:47.225683Z","iopub.status.idle":"2022-03-26T21:09:47.285173Z","shell.execute_reply.started":"2022-03-26T21:09:47.225639Z","shell.execute_reply":"2022-03-26T21:09:47.284239Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"markdown","source":"## Exploratory Data Analysis & Data Cleaning\n\nWe wanted to get the **Month** and **Year** to be a single date variable. Additionally, we wanted to check for anomalies that might affect the model that we build. We check for the summary statistics as follows:","metadata":{}},{"cell_type":"code","source":"df.describe()","metadata":{"execution":{"iopub.status.busy":"2022-03-26T15:04:56.436908Z","iopub.execute_input":"2022-03-26T15:04:56.437613Z","iopub.status.idle":"2022-03-26T15:04:56.484376Z","shell.execute_reply.started":"2022-03-26T15:04:56.437562Z","shell.execute_reply":"2022-03-26T15:04:56.483698Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"markdown","source":"THe price of wheat seems pretty reasonable, except that the maximum price may be a bit **too** high. It is almost twice as much as the mean price and the 75th percentile price. However, we would not do anything to correct or remove the highest price, as it might be essential to our analysis later on.\n\n### Converting the Year and Month into a Date Format\n\nNow, we will have a variable **YearMonth**, that merge the two date data columns (**Year** and **Month**) into one.","metadata":{}},{"cell_type":"code","source":"from time import strptime\ndf['Month']\n\ndf['YearMonth'] = pd.date_range('1992-02','2022-01',freq='MS').strftime('%Y-%m')\ndf.head()\n\n","metadata":{"execution":{"iopub.status.busy":"2022-03-26T21:13:46.907121Z","iopub.execute_input":"2022-03-26T21:13:46.907477Z","iopub.status.idle":"2022-03-26T21:13:46.949275Z","shell.execute_reply.started":"2022-03-26T21:13:46.907439Z","shell.execute_reply":"2022-03-26T21:13:46.948423Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"markdown","source":"### Taking care of Null Values\n\nNow that we have that taken care of, we would need to further analyze and see if there were any null values, and take care of it before we run any models.","metadata":{}},{"cell_type":"code","source":"df1 = df[df.isna().any(axis=1)]\ndf1","metadata":{"execution":{"iopub.status.busy":"2022-03-26T15:05:01.523641Z","iopub.execute_input":"2022-03-26T15:05:01.524153Z","iopub.status.idle":"2022-03-26T15:05:01.544931Z","shell.execute_reply.started":"2022-03-26T15:05:01.524111Z","shell.execute_reply":"2022-03-26T15:05:01.544138Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"markdown","source":"We see that the last row and the most recent **Wheat Price in Tons** data point has a null value. Instead of taking care of the anomaly by having the null value of the **Wheat Price in Tons** of January 2022 be **zero**, we will drop this entry, since it won't create a break in time with the lack of this variable, and won't affect our model predictions. We are only targeting the predictability of the year **2021** with the data from previous years dating back to **1992**.\n\nAlong with dropping the row with the null value, we are going to add another variable, **YMD**, that gives the actual beginning date of each month and year in case the model we are working with only accepts a certain format.","metadata":{}},{"cell_type":"code","source":"df=pd.DataFrame.dropna(df)\n\ndf['YMD'] = pd.date_range('1992-02','2021-12',freq='MS')","metadata":{"execution":{"iopub.status.busy":"2022-03-26T21:13:49.862230Z","iopub.execute_input":"2022-03-26T21:13:49.862904Z","iopub.status.idle":"2022-03-26T21:13:49.885542Z","shell.execute_reply.started":"2022-03-26T21:13:49.862846Z","shell.execute_reply":"2022-03-26T21:13:49.884655Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"markdown","source":"## Detecting Trends\n\nAfter finishing with data cleaning, we should move to see some of the data trends. With a **moving average plot**, we can smooth out any short-term fluctuations in the series so that only long-term changes remain.\n\nWhen computing a moving average of a time series, we take the average values within a sliding window of a defined width. We will start with a 6-month moving average graph, followed by a 1-year moving average.","metadata":{}},{"cell_type":"code","source":"test1 = df.Price_wheat_ton.rolling(\n    window=6,       # 6 month window, or a 6 month moving average\n    center=True,      # puts the average at the center of the window\n).mean()              # compute the mean (could also do median, std, min, max, ...)\n\ntest1.plot()\n\ndf.Price_wheat_ton.plot(title=\"Wheat Prices in Tons - 6-Month Moving Average\", xlabel='Number of Months from Feb. 1992', ylabel='Price' )","metadata":{"execution":{"iopub.status.busy":"2022-03-26T15:06:51.840706Z","iopub.execute_input":"2022-03-26T15:06:51.841148Z","iopub.status.idle":"2022-03-26T15:06:52.297248Z","shell.execute_reply.started":"2022-03-26T15:06:51.841115Z","shell.execute_reply":"2022-03-26T15:06:52.296324Z"},"trusted":true},"execution_count":14,"outputs":[]},{"cell_type":"code","source":"test2 = df.Price_wheat_ton.rolling(\n    window=12,       # 6 month window, or a 6 month moving average\n    center=True,      # puts the average at the center of the window\n).mean()              # compute the mean (could also do median, std, min, max, ...)\n\ntest2.plot()\n\ndf.Price_wheat_ton.plot(title=\"Wheat Prices in Tons - 12-Month Moving Average\", xlabel='Number of Months from Feb. 1992', ylabel='Price')","metadata":{"execution":{"iopub.status.busy":"2022-03-26T15:05:25.305292Z","iopub.execute_input":"2022-03-26T15:05:25.306246Z","iopub.status.idle":"2022-03-26T15:05:25.766919Z","shell.execute_reply.started":"2022-03-26T15:05:25.306200Z","shell.execute_reply":"2022-03-26T15:05:25.765964Z"},"trusted":true},"execution_count":10,"outputs":[]},{"cell_type":"markdown","source":"We can see that the trend has no clear pattern and fluctuates similar to a polynomial regression, meaning that we are working with non-stationary data. \n\nWhat happens if we do a **50-month** moving average? We should test this out!","metadata":{}},{"cell_type":"code","source":"test3 = df.Price_wheat_ton.rolling(\n    window=50,       # 6 month window, or a 6 month moving average\n    center=True,      # puts the average at the center of the window\n).mean()              # compute the mean (could also do median, std, min, max, ...)\n\ntest3.plot()\n\ndf.Price_wheat_ton.plot(title=\"Wheat Prices in Tons - 50-Month Moving Average\", xlabel='Number of Months from Feb. 1992', ylabel='Price')","metadata":{"execution":{"iopub.status.busy":"2022-03-26T15:14:03.455064Z","iopub.execute_input":"2022-03-26T15:14:03.455463Z","iopub.status.idle":"2022-03-26T15:14:03.909560Z","shell.execute_reply.started":"2022-03-26T15:14:03.455424Z","shell.execute_reply":"2022-03-26T15:14:03.908513Z"},"trusted":true},"execution_count":16,"outputs":[]},{"cell_type":"markdown","source":"## What we can determine from the trend\n\nThis has a clearer polynomial linear regression trend than was presented compared to when we do a 6-month or 12-month average. Since we've identified the shape of the trend, we can attempt to model it using a time-step feature. \n\nWe can determine that the data that we are working with is non-stationary data, meaning that the statistical measures, such as the mean, standard deviation, and auto correlation show a decreasing or increasing trend over time. In this case, it is an increasing trend over time.\n\n## Detecting Seasonality\n\nWe can also choose to decompose different parts of the time series with **seasonal_decompose** from the **statsmodels** package. As you can see, from the graphical example below, we analyze that:\n* The trend is fluctuating, showing a series of increase and decrease over time, but moving towards increase.\n* For the seasonal portion of the data, we see more of a yearly pattern.\n* As for the residual portion of the data, this isthe portion that cannot be explained by the trend or seasons. It is the noise portion of the dataset.\n","metadata":{}},{"cell_type":"code","source":"df1 = df[['Price_wheat_ton','YMD']]\ndf1.set_index('YMD',inplace=True)\nfrom statsmodels.tsa.seasonal import seasonal_decompose\ntrend_detect=seasonal_decompose(df1['Price_wheat_ton'])\ntrend_detect.plot();","metadata":{"execution":{"iopub.status.busy":"2022-03-26T21:13:56.165535Z","iopub.execute_input":"2022-03-26T21:13:56.165883Z","iopub.status.idle":"2022-03-26T21:13:57.412257Z","shell.execute_reply.started":"2022-03-26T21:13:56.165848Z","shell.execute_reply":"2022-03-26T21:13:57.411247Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"markdown","source":"We shall add another variable, **Time**, which dates the number of months from the beginning of the series (February 1992), in case of further analysis.","metadata":{}},{"cell_type":"code","source":"df['Time'] = np.arange(len(df.index))\n# df['Lag_1_W'] = df['Price_wheat_ton'].shift(1)","metadata":{"execution":{"iopub.status.busy":"2022-03-26T21:13:58.441521Z","iopub.execute_input":"2022-03-26T21:13:58.442423Z","iopub.status.idle":"2022-03-26T21:13:58.450159Z","shell.execute_reply.started":"2022-03-26T21:13:58.442365Z","shell.execute_reply":"2022-03-26T21:13:58.448859Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"markdown","source":"# Model Making\n\nWe can choose to make the data stationary for the machine to better detect the trend and the pattern. However, I have decided to use models that were geared towards non-stationary data. Since we are using non-stationary data, we wouldn't use some of the models geared toward stationary data with a constant mean and variance, such as the **ARIMA** model. \n\n## Simple First Order Linear Regression\nWe fit a simple first order linear regression in the data. You can tell that it does not do well, since it has a polynomial linear trend.","metadata":{}},{"cell_type":"code","source":"fig, ax = plt.subplots()\nax.plot('Time', 'Price_wheat_ton', data=df, color='0.75')\nax = sns.regplot(x='Time', y='Price_wheat_ton', data=df, ci=None, scatter_kws=dict(color='0.25'))\nax.set_title('Time Plot of Wheat Price per Ton');","metadata":{"execution":{"iopub.status.busy":"2022-03-26T17:22:55.059305Z","iopub.execute_input":"2022-03-26T17:22:55.059634Z","iopub.status.idle":"2022-03-26T17:22:55.515550Z","shell.execute_reply.started":"2022-03-26T17:22:55.059603Z","shell.execute_reply":"2022-03-26T17:22:55.514562Z"},"trusted":true},"execution_count":24,"outputs":[]},{"cell_type":"markdown","source":"## Using Deterministic Process and Trends\n\n**Deterministic trends** are constant increases in the mean of the series over time, though the variable may fluctuate above or below its trend line randomly. It is a non-stationary process, since it violates the stationarity assumptions due to its variable being unable to revert toward a fixed mean after any shock.\n\nWe will use **DeterministicProcess** to create features for dates. It is a technical term for a time series that is non-random or completely determined. Features derived from the time index will generally be deterministic.","metadata":{}},{"cell_type":"code","source":"from statsmodels.tsa.deterministic import DeterministicProcess\ny = df[\"Price_wheat_ton\"]  # the target\n\ndp = DeterministicProcess(\n    index=df.YearMonth,  # dates from the training data\n    constant=True,       # dummy feature for the bias (y_intercept)\n    order=5,             # the time dummy (trend)\n    drop=True,           # drop terms if necessary to avoid collinearity\n)\n# `in_sample` creates features for the dates given in the `index` argument\nX = dp.in_sample()\n\nX.head()","metadata":{"execution":{"iopub.status.busy":"2022-03-26T21:14:24.130094Z","iopub.execute_input":"2022-03-26T21:14:24.130436Z","iopub.status.idle":"2022-03-26T21:14:24.190328Z","shell.execute_reply.started":"2022-03-26T21:14:24.130398Z","shell.execute_reply":"2022-03-26T21:14:24.189022Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"markdown","source":"## Polynomial Linear Regression \n\nAfter our **DeterministicProcess** feature creation, we shall be running a polynomial linear regression with the feature that we have.","metadata":{}},{"cell_type":"code","source":"from sklearn.linear_model import LinearRegression\n\ny = df[\"Price_wheat_ton\"]  # the target\n# The intercept is the same as the `const` feature from\n# DeterministicProcess. LinearRegression behaves badly with duplicated\n# features, so we need to be sure to exclude it here.\nmodel = LinearRegression()\nmodel.fit(X, y)\n\ny_pred = pd.Series(model.predict(X), index=X.index)","metadata":{"execution":{"iopub.status.busy":"2022-03-26T21:14:30.201236Z","iopub.execute_input":"2022-03-26T21:14:30.201523Z","iopub.status.idle":"2022-03-26T21:14:30.219106Z","shell.execute_reply.started":"2022-03-26T21:14:30.201493Z","shell.execute_reply":"2022-03-26T21:14:30.217922Z"},"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"markdown","source":"We will use the **root-mean-square error (RMSE)** to measure the differences between values (sample or population values) predicted by a model or an estimator and the values observed from the dataset.","metadata":{}},{"cell_type":"code","source":"from sklearn.metrics import mean_squared_error\n\nrmse=(mean_squared_error(df['Price_wheat_ton'],y_pred))\nprint(rmse)","metadata":{"execution":{"iopub.status.busy":"2022-03-26T21:14:38.425003Z","iopub.execute_input":"2022-03-26T21:14:38.426086Z","iopub.status.idle":"2022-03-26T21:14:38.432834Z","shell.execute_reply.started":"2022-03-26T21:14:38.426038Z","shell.execute_reply":"2022-03-26T21:14:38.432015Z"},"trusted":true},"execution_count":10,"outputs":[]},{"cell_type":"markdown","source":"### Linear Regression with Deterministic Process (Order of 5)\n\nAs you can see, when fitting a 5th order linear regression model into the wheat price data, this regression does seem like a good fit from a graphical standpoint. It looks similar to the **50-Month Moving Average Plot** that we created in the trend portion of the project.","metadata":{}},{"cell_type":"code","source":"\nax = df.Price_wheat_ton.plot(style=\".\", color=\"0.5\", title=\"Wheat Price - Linear Trend\")\n_ = y_pred.plot(ax=ax, linewidth=3, label=\"Trend\")","metadata":{"execution":{"iopub.status.busy":"2022-03-26T17:31:39.608561Z","iopub.execute_input":"2022-03-26T17:31:39.609257Z","iopub.status.idle":"2022-03-26T17:31:40.030449Z","shell.execute_reply.started":"2022-03-26T17:31:39.609205Z","shell.execute_reply":"2022-03-26T17:31:40.029440Z"},"trusted":true},"execution_count":28,"outputs":[]},{"cell_type":"markdown","source":"So what happens if you tried predicting the next set of prices?","metadata":{}},{"cell_type":"code","source":"X = dp.out_of_sample(steps=12)\n\ny_fore = pd.Series(model.predict(X), index=X.index)\n\nax = df.Price_wheat_ton.plot(title=\"Wheat Price - Linear Trend\", **plot_params)\nax = y_pred.plot(ax=ax, linewidth=3, label=\"Trend\")\nax = y_fore.plot(ax=ax, linewidth=3, label=\"Trend Forecast\", color=\"C3\")\n_ = ax.legend()","metadata":{"execution":{"iopub.status.busy":"2022-03-26T17:33:42.417184Z","iopub.execute_input":"2022-03-26T17:33:42.418192Z","iopub.status.idle":"2022-03-26T17:33:42.914493Z","shell.execute_reply.started":"2022-03-26T17:33:42.418060Z","shell.execute_reply":"2022-03-26T17:33:42.913635Z"},"trusted":true},"execution_count":29,"outputs":[]},{"cell_type":"markdown","source":"Now, the trend forecast might go a bit too high, maybe higher than the maximum historical wheat price per ton. The prediction is likely to be too steep of an increase from what the actual might look like. Although it was able to predict the direction of the trend, the actual might not have a pricing increase as fast as predicted. \n\n### Linear Regression with Deterministic Process (Order of 7)\n\nWe are going to look at what happens when fitting a higher order of linear regression model (a 7th order linear regression).","metadata":{}},{"cell_type":"code","source":"dp = DeterministicProcess(\n    index=df.YearMonth,  # dates from the training data\n    constant=True,       # dummy feature for the bias (y_intercept)\n    order=7,             # the time dummy (trend)\n    drop=True,           # drop terms if necessary to avoid collinearity\n)\n# `in_sample` creates features for the dates given in the `index` argument\nX = dp.in_sample()\n\ny = df[\"Price_wheat_ton\"]  # the target\n# The intercept is the same as the `const` feature from\n# DeterministicProcess. LinearRegression behaves badly with duplicated\n# features, so we need to be sure to exclude it here.\nmodel = LinearRegression()\nmodel.fit(X, y)\n\ny_pred = pd.Series(model.predict(X), index=X.index)\n\nax = df.Price_wheat_ton.plot(style=\".\", color=\"0.5\", title=\"Wheat Price - Linear Trend\")\n_ = y_pred.plot(ax=ax, linewidth=3, label=\"Trend\")","metadata":{"execution":{"iopub.status.busy":"2022-03-26T17:37:33.143934Z","iopub.execute_input":"2022-03-26T17:37:33.144799Z","iopub.status.idle":"2022-03-26T17:37:33.569369Z","shell.execute_reply.started":"2022-03-26T17:37:33.144752Z","shell.execute_reply":"2022-03-26T17:37:33.568147Z"},"trusted":true},"execution_count":30,"outputs":[]},{"cell_type":"code","source":"from sklearn.metrics import mean_squared_error\n\nrmse=(mean_squared_error(df['Price_wheat_ton'],y_pred))\nprint(rmse)","metadata":{"execution":{"iopub.status.busy":"2022-03-26T18:52:30.412638Z","iopub.execute_input":"2022-03-26T18:52:30.413041Z","iopub.status.idle":"2022-03-26T18:52:30.418264Z","shell.execute_reply.started":"2022-03-26T18:52:30.413012Z","shell.execute_reply":"2022-03-26T18:52:30.417659Z"},"trusted":true},"execution_count":15,"outputs":[]},{"cell_type":"markdown","source":"Now, when fitting a 7th order linear regression model into the wheat price data, this regression is not as much of a good fit as compared to the 5th order linear regression model, as shown that the RMSE is higher in the 7th order regression model. Some of the price fluctuations that we see between 2005 and 2017 isn't as detailed as we see with the 5th order polynomial regression. \n\nHowever, we should see how the 7th order polynomial regression predicts later time points.","metadata":{}},{"cell_type":"code","source":"X = dp.out_of_sample(steps=12)\n\ny_fore = pd.Series(model.predict(X), index=X.index)\n\nax = df.Price_wheat_ton.plot(title=\"Wheat Price - Linear Trend\", **plot_params)\nax = y_pred.plot(ax=ax, linewidth=3, label=\"Trend\")\nax = y_fore.plot(ax=ax, linewidth=3, label=\"Trend Forecast\", color=\"C3\")\n_ = ax.legend()","metadata":{"execution":{"iopub.status.busy":"2022-03-26T17:44:56.459743Z","iopub.execute_input":"2022-03-26T17:44:56.460165Z","iopub.status.idle":"2022-03-26T17:44:56.975892Z","shell.execute_reply.started":"2022-03-26T17:44:56.460121Z","shell.execute_reply":"2022-03-26T17:44:56.974749Z"},"trusted":true},"execution_count":31,"outputs":[]},{"cell_type":"markdown","source":"We can see that the 7th order polynomial linear regression also predicts a price increase, though it is not as steep and exponential as compared to the 5th order polynomial regression. The *direction of the trend* prediction is the same, but the **rate of increase differs**, and we can agree on that the 7th order polynomial linear regression shows a more reasonable prediction, but the 5th order polynomial linear regression gives a better model of the historical data.","metadata":{}},{"cell_type":"markdown","source":"## Linear Regression with Fourier Features\n\nWe wanted to choose seasonal features to see if it will add better predictive capability to our model. If we treat a seasonal period as a categorical feature and apply one-hot encoding, would it give a better model to our polynomial linear regression?\n\nWe will do a **periodogram** to see how many Fourier pairs we should actually include in our features, since it tells us the strength of the frequencies in a time series. ","metadata":{}},{"cell_type":"code","source":"plt.style.use(\"seaborn-whitegrid\")\nplt.rc(\"figure\", autolayout=True, figsize=(11, 5))\nplt.rc(\n    \"axes\",\n    labelweight=\"bold\",\n    labelsize=\"large\",\n    titleweight=\"bold\",\n    titlesize=16,\n    titlepad=10,\n)\nplot_params = dict(\n    color=\"0.75\",\n    style=\".-\",\n    markeredgecolor=\"0.25\",\n    markerfacecolor=\"0.25\",\n    legend=False,\n)\n%config InlineBackend.figure_format = 'retina'\n\n\n# annotations: https://stackoverflow.com/a/49238256/5769929\ndef seasonal_plot(X, y, period, freq, ax=None):\n    if ax is None:\n        _, ax = plt.subplots()\n    palette = sns.color_palette(\"husl\", n_colors=X[period].nunique(),)\n    ax = sns.lineplot(\n        x=freq,\n        y=y,\n        hue=period,\n        data=X,\n        ci=False,\n        ax=ax,\n        palette=palette,\n        legend=False,\n    )\n    ax.set_title(f\"Seasonal Plot ({period}/{freq})\")\n    for line, name in zip(ax.lines, X[period].unique()):\n        y_ = line.get_ydata()[-1]\n        ax.annotate(\n            name,\n            xy=(1, y_),\n            xytext=(6, 0),\n            color=line.get_color(),\n            xycoords=ax.get_yaxis_transform(),\n            textcoords=\"offset points\",\n            size=14,\n            va=\"center\",\n        )\n    return ax\n\n\ndef plot_periodogram(ts, detrend='linear', ax=None):\n    from scipy.signal import periodogram\n    fs = pd.Timedelta(\"1Y\") / pd.Timedelta(\"1D\")\n    freqencies, spectrum = periodogram(\n        ts,\n        fs=fs,\n        detrend=detrend,\n        window=\"boxcar\",\n        scaling='spectrum',\n    )\n    if ax is None:\n        _, ax = plt.subplots()\n    ax.step(freqencies, spectrum, color=\"purple\")\n    ax.set_xscale(\"log\")\n    ax.set_xticks([1, 2, 4, 6, 12, 26, 52, 104])\n    ax.set_xticklabels(\n        [\n            \"Annual (1)\",\n            \"Semiannual (2)\",\n            \"Quarterly (4)\",\n            \"Bimonthly (6)\",\n            \"Monthly (12)\",\n            \"Biweekly (26)\",\n            \"Weekly (52)\",\n            \"Semiweekly (104)\",\n        ],\n        rotation=30,\n    )\n    ax.ticklabel_format(axis=\"y\", style=\"sci\", scilimits=(0, 0))\n    ax.set_ylabel(\"Variance\")\n    ax.set_title(\"Periodogram\")\n    return ax\n\n","metadata":{"execution":{"iopub.status.busy":"2022-03-26T18:03:02.653962Z","iopub.execute_input":"2022-03-26T18:03:02.654296Z","iopub.status.idle":"2022-03-26T18:03:02.695243Z","shell.execute_reply.started":"2022-03-26T18:03:02.654253Z","shell.execute_reply":"2022-03-26T18:03:02.694189Z"},"trusted":true},"execution_count":44,"outputs":[]},{"cell_type":"code","source":"plot_periodogram(df.Price_wheat_ton);","metadata":{"execution":{"iopub.status.busy":"2022-03-26T18:03:04.787170Z","iopub.execute_input":"2022-03-26T18:03:04.787676Z","iopub.status.idle":"2022-03-26T18:03:05.385416Z","shell.execute_reply.started":"2022-03-26T18:03:04.787597Z","shell.execute_reply":"2022-03-26T18:03:05.384388Z"},"trusted":true},"execution_count":45,"outputs":[]},{"cell_type":"markdown","source":"### Periodogram Analysis\n\nFrom left to right, the periodogram drops off after Annually and Semiannually. \nIt shows a strong annual season, which means that we can model the annual season with Fourier features. From right to left, the periodogram falls off around Monthly (12), so let's use 12 Fourier pairs.\n\n\n**Fourier features** try to capture the overall shape of the seasonal curve with just a few features instead of creating a feature for each date. By using Fourier features, we can capture frequencies within a season and include in our training data the periodic curves of similar frequencies as the season we are trying to model. \n\nWe'll create our seasonal features using DeterministicProcess. ","metadata":{}},{"cell_type":"code","source":"from statsmodels.tsa.deterministic import CalendarFourier\ny = df.Price_wheat_ton\n\nfourier = CalendarFourier(freq=\"A\", order=12)  # 10 sin/cos pairs for \"A\"nnual seasonality\n\ndp = DeterministicProcess(\n    index=df.YMD,\n    constant=True,               # dummy feature for bias (y-intercept)\n    order=5,                     # trend (order 1 means linear)\n    seasonal=True,               \n    additional_terms=[fourier],  # annual seasonality (fourier)\n    drop=True,                   # drop terms to avoid collinearity\n)\n\nX = dp.in_sample() ","metadata":{"execution":{"iopub.status.busy":"2022-03-26T18:55:33.811008Z","iopub.execute_input":"2022-03-26T18:55:33.811753Z","iopub.status.idle":"2022-03-26T18:55:33.853723Z","shell.execute_reply.started":"2022-03-26T18:55:33.811710Z","shell.execute_reply":"2022-03-26T18:55:33.852767Z"},"trusted":true},"execution_count":22,"outputs":[]},{"cell_type":"code","source":"model = LinearRegression().fit(X, y)\ny_pred = pd.Series(\n    model.predict(X),\n    index=X.index,\n    name='Fitted',\n)\n\ny_pred = pd.Series(model.predict(X), index=X.index)\nax = df1.plot(color='0.25', style='.', title=\"Wheat Price - Seasonal Forecast with Fourier\")\nax = y_pred.plot( label=\"Seasonal\")\nax.legend();","metadata":{"execution":{"iopub.status.busy":"2022-03-26T18:55:34.924666Z","iopub.execute_input":"2022-03-26T18:55:34.924936Z","iopub.status.idle":"2022-03-26T18:55:35.445906Z","shell.execute_reply.started":"2022-03-26T18:55:34.924905Z","shell.execute_reply":"2022-03-26T18:55:35.445067Z"},"trusted":true},"execution_count":23,"outputs":[]},{"cell_type":"code","source":"rmse=(mean_squared_error(df['Price_wheat_ton'],y_pred))\nprint(rmse)","metadata":{"execution":{"iopub.status.busy":"2022-03-26T18:55:36.379075Z","iopub.execute_input":"2022-03-26T18:55:36.379626Z","iopub.status.idle":"2022-03-26T18:55:36.385335Z","shell.execute_reply.started":"2022-03-26T18:55:36.379587Z","shell.execute_reply":"2022-03-26T18:55:36.384567Z"},"trusted":true},"execution_count":24,"outputs":[]},{"cell_type":"markdown","source":"From the graph and the root mean square error above, using the Fourier feature gives better predictability as compared to the polynomial linear regression that we ran previously, since it took seasonality into account. \n\n\n### Predictive capability of using Fourier Features\n\nWe will run a test to see how the model predicts the last 12 months of data using the prior historical data from before 2021. ","metadata":{}},{"cell_type":"code","source":"X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=12, shuffle=False)\n\n# Fit and predict\nmodel = LinearRegression()  # `fit_intercept=True` since we didn't use DeterministicProcess\nmodel.fit(X_train, y_train)\ny_pred = pd.Series(model.predict(X_train), index=y_train.index)\ny_fore = pd.Series(model.predict(X_test), index=y_test.index)","metadata":{"execution":{"iopub.status.busy":"2022-03-26T19:10:31.576561Z","iopub.execute_input":"2022-03-26T19:10:31.576872Z","iopub.status.idle":"2022-03-26T19:10:31.591045Z","shell.execute_reply.started":"2022-03-26T19:10:31.576845Z","shell.execute_reply":"2022-03-26T19:10:31.590347Z"},"trusted":true},"execution_count":25,"outputs":[]},{"cell_type":"code","source":"ax = y_train.plot(**plot_params)\nax = y_test.plot(**plot_params)\nax = y_pred.plot(ax=ax, title=\"Wheat Prices in Tons over Time\", xlabel= \"Number of Months after Feb 1992\", ylabel=\"Price\")\n_ = y_fore.plot(ax=ax, color='C3')","metadata":{"execution":{"iopub.status.busy":"2022-03-26T19:11:32.777645Z","iopub.execute_input":"2022-03-26T19:11:32.778258Z","iopub.status.idle":"2022-03-26T19:11:33.227094Z","shell.execute_reply.started":"2022-03-26T19:11:32.778221Z","shell.execute_reply":"2022-03-26T19:11:33.226273Z"},"trusted":true},"execution_count":27,"outputs":[]},{"cell_type":"markdown","source":"We are certain that, with **Red** being the predicted result, and **Blue** as the training model, and **Black** are the actual prices in 2021, this is a good predictive model compared to the previous polynomial regression model that did not use Fourier features. ","metadata":{}},{"cell_type":"markdown","source":"## Using Splines Instead\n\nNow with splines, or **multivariate adaptive regression splines (MARS)**, we can form a non-parametric linear regression model that automatically models nonlinearities and interactions between variables. Let's see how good it models the price of wheat over time.","metadata":{}},{"cell_type":"code","source":"from pyearth import Earth\n\n# Target and features are the same as before\ny = df[\"Price_wheat_ton\"].copy()\ndp = DeterministicProcess(index=y.index, order=1)\nX = dp.in_sample()\n\n# Fit a MARS model with `Earth`\nmodel = Earth()\nmodel.fit(X, y)\n\ny_pred = pd.Series(model.predict(X), index=X.index)\n\nax = y.plot(**plot_params, title=\"Price of Wheat over time from 1992 (in months)\", ylabel=\"Price\")\nax = y_pred.plot(ax=ax, linewidth=3, label=\"Trend\")","metadata":{"execution":{"iopub.status.busy":"2022-03-26T19:15:54.067951Z","iopub.execute_input":"2022-03-26T19:15:54.068276Z","iopub.status.idle":"2022-03-26T19:15:55.861211Z","shell.execute_reply.started":"2022-03-26T19:15:54.068242Z","shell.execute_reply":"2022-03-26T19:15:55.860393Z"},"trusted":true},"execution_count":28,"outputs":[]},{"cell_type":"code","source":"rmse=(mean_squared_error(df['Price_wheat_ton'],y_pred))\nprint(rmse)","metadata":{"execution":{"iopub.status.busy":"2022-03-26T19:16:40.759826Z","iopub.execute_input":"2022-03-26T19:16:40.760124Z","iopub.status.idle":"2022-03-26T19:16:40.766716Z","shell.execute_reply.started":"2022-03-26T19:16:40.760092Z","shell.execute_reply":"2022-03-26T19:16:40.765840Z"},"trusted":true},"execution_count":29,"outputs":[]},{"cell_type":"markdown","source":"We can see from the RMSE that MARS models better than polynomial regression. Will it have better predictive capability using pre-2021 data to determine the data for 2021?","metadata":{}},{"cell_type":"code","source":"X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=12, shuffle=False)\n\n# Fit and predict\nmodel = Earth()  # `fit_intercept=True` since we didn't use DeterministicProcess\nmodel.fit(X_train, y_train)\ny_pred = pd.Series(model.predict(X_train), index=y_train.index)\ny_fore = pd.Series(model.predict(X_test), index=y_test.index)\n\nax = y_train.plot(**plot_params)\nax = y_test.plot(**plot_params)\nax = y_pred.plot(ax=ax, title=\"Wheat Prices in Tons over Time\", xlabel= \"Number of Months after Feb 1992\", ylabel=\"Price\")\n_ = y_fore.plot(ax=ax, color='C3')","metadata":{"execution":{"iopub.status.busy":"2022-03-26T19:22:42.481635Z","iopub.execute_input":"2022-03-26T19:22:42.481950Z","iopub.status.idle":"2022-03-26T19:22:43.997712Z","shell.execute_reply.started":"2022-03-26T19:22:42.481914Z","shell.execute_reply":"2022-03-26T19:22:43.996860Z"},"trusted":true},"execution_count":30,"outputs":[]},{"cell_type":"markdown","source":"We can see that, compared to the polynomial regression model (5th order) using the Fourier Feature, the MARS model **have better modeling capability**, but **doesn't have a better predictive capability** in terms of using dataset from 1992 to 2020 to predict the actual data in 2021.\n\n## Multistep Forecasting with Lags\n\nWith a **multioutput model**, we can produce a model that gives multiple outputs naturally. There are a number of strategies for producing the multiple target steps required for a forecast, but we will focus on using the direct strategy and a combination of direct and recursive strategies.\n\nAs described from [Forecasting with Machine Learning](https://www.kaggle.com/code/ryanholbrook/forecasting-with-machine-learning), the definition of each strategy is as follows:\n\n* **Direct strategy**: Train a separate model for each step in the horizon: one model forecasts 1-step ahead, another 2-steps ahead, and so on. Forecasting 1-step ahead is a different problem than 2-steps ahead (and so on). However, training lots of models can be computationally expensive.\n\n* **Recursive strategy**: Train a single one-step model and use its forecasts to update the lag features for the next step. With the recursive method, we feed a model's 1-step forecast back in to that same model to use as a lag feature for the next forecasting step. We only need to train one model, but since errors will propagate from step to step, forecasts can be inaccurate for long horizons.\n\n* **DirRec strategy**: A combination of the direct and recursive strategies: train a model for each step and use forecasts from previous steps as new lag features. Step by step, each model gets an additional lag input. Since each model always has an up-to-date set of lag features, the DirRec strategy can capture serial dependence better than Direct, but it can also suffer from error propagation like Recursive.\n\n### Start by making Lag Features\n\nSo we will start by making lag features. A lag 1 feature shifts the time series forward 1 step, which means you could forecast 1 step into the future, but not 2 steps.","metadata":{}},{"cell_type":"code","source":"from pathlib import Path\nimport ipywidgets as widgets\nfrom learntools.time_series.style import *  # plot style settings\nfrom learntools.time_series.utils import (create_multistep_example,\n                                          load_multistep_data,\n                                          make_lags,\n                                          make_multistep_target,\n                                          plot_multistep)\n\n\ny = df.loc[:, 'Price_wheat_ton']\n\n#  Make 6 lag features\nX = make_lags(y, lags=6).dropna()\n\n# Make multistep target\ny = make_multistep_target(y, steps=12).dropna()\n\ny, X = y.align(X, join='inner', axis=0)","metadata":{"execution":{"iopub.status.busy":"2022-03-26T21:19:02.573942Z","iopub.execute_input":"2022-03-26T21:19:02.574994Z","iopub.status.idle":"2022-03-26T21:19:02.634758Z","shell.execute_reply.started":"2022-03-26T21:19:02.574944Z","shell.execute_reply":"2022-03-26T21:19:02.633849Z"},"trusted":true},"execution_count":11,"outputs":[]},{"cell_type":"markdown","source":"### Direct Strategy\n\nWe will use a multioutput Direct strategy to test the model fit.","metadata":{}},{"cell_type":"code","source":"from sklearn.multioutput import MultiOutputRegressor\nfrom xgboost import XGBRegressor\nfrom sklearn.multioutput import RegressorChain\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.model_selection import train_test_split\n\n\n# Create splits\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, shuffle=False)\n\nmodel = MultiOutputRegressor(XGBRegressor())\nmodel.fit(X_train, y_train)\n\ny_fit = pd.DataFrame(model.predict(X_train), index=X_train.index, columns=y.columns)\ny_pred = pd.DataFrame(model.predict(X_test), index=X_test.index, columns=y.columns)\n\ntrain_rmse = mean_squared_error(y_train, y_fit, squared=False)\ntest_rmse = mean_squared_error(y_test, y_pred, squared=False)\nprint((f\"Train RMSE: {train_rmse:.2f}\\n\" f\"Test RMSE: {test_rmse:.2f}\"))\n","metadata":{"execution":{"iopub.status.busy":"2022-03-26T21:20:26.231128Z","iopub.execute_input":"2022-03-26T21:20:26.232163Z","iopub.status.idle":"2022-03-26T21:20:30.217025Z","shell.execute_reply.started":"2022-03-26T21:20:26.232071Z","shell.execute_reply":"2022-03-26T21:20:30.216320Z"},"trusted":true},"execution_count":14,"outputs":[]},{"cell_type":"markdown","source":"### DirRec Strategy\n\nWe will use a multioutput DirRec strategy to test the model fit.","metadata":{}},{"cell_type":"code","source":"\n\nmodel = RegressorChain(XGBRegressor())\nmodel.fit(X_train, y_train)\n\ny_fit = pd.DataFrame(model.predict(X_train), index=X_train.index, columns=y.columns)\ny_pred = pd.DataFrame(model.predict(X_test), index=X_test.index, columns=y.columns)\n\ntrain_rmse = mean_squared_error(y_train, y_fit, squared=False)\ntest_rmse = mean_squared_error(y_test, y_pred, squared=False)\nprint((f\"Train RMSE: {train_rmse:.2f}\\n\" f\"Test RMSE: {test_rmse:.2f}\"))\n\n","metadata":{"execution":{"iopub.status.busy":"2022-03-26T21:32:40.510326Z","iopub.execute_input":"2022-03-26T21:32:40.511390Z","iopub.status.idle":"2022-03-26T21:32:46.229179Z","shell.execute_reply.started":"2022-03-26T21:32:40.511314Z","shell.execute_reply":"2022-03-26T21:32:46.228460Z"},"trusted":true},"execution_count":15,"outputs":[]},{"cell_type":"markdown","source":"### Direct and DirRec Strategy Comparison\n\nWe can see, from the RMSE analysis above, that using the Direct strategy to model Wheat Prices is more accurate and closer to the actual Wheat Price than using the DirRec strategy.","metadata":{}},{"cell_type":"markdown","source":"## Neural Networks\n\nWe wanted to try another way- neural networks, to see if it is possible to train the computer to assign different weights and predict better than linear regression models, splines, or multistep direct recursive model.\n\n### Scaling\n\nWe will scale the training set (1992-2020 data) and the testing set (2021 data) from 0 to 1 to better fit in the neural network.","metadata":{}},{"cell_type":"code","source":"train= df1.iloc[:-12]\ntest= df1.iloc[-12:]\n\nfrom sklearn.preprocessing import MinMaxScaler\nscaler=MinMaxScaler()\n\n#scale the data from 0 to 1\n\nscaler.fit(train)\nscaled_train= scaler.transform(train)\nscaled_test= scaler.transform(test)","metadata":{"execution":{"iopub.status.busy":"2022-03-26T22:13:54.093987Z","iopub.execute_input":"2022-03-26T22:13:54.094420Z","iopub.status.idle":"2022-03-26T22:13:54.109789Z","shell.execute_reply.started":"2022-03-26T22:13:54.094381Z","shell.execute_reply":"2022-03-26T22:13:54.108452Z"},"trusted":true},"execution_count":16,"outputs":[]},{"cell_type":"markdown","source":"### Generators and Features\n\nWith neural network, we would be defining the generators and features of the model. The number of features that we have is only 1, which comes from the time variable that we have. However, we can set the number of input for the generator matrix.","metadata":{}},{"cell_type":"code","source":"from keras.preprocessing.sequence import TimeseriesGenerator\n\n#define the generators and features\ninput_n= 6 #6 months\nfeatures= 1\ngenerator= TimeseriesGenerator(scaled_train, scaled_train, length=input_n, batch_size=1)","metadata":{"execution":{"iopub.status.busy":"2022-03-26T23:10:30.756239Z","iopub.execute_input":"2022-03-26T23:10:30.756744Z","iopub.status.idle":"2022-03-26T23:10:30.760803Z","shell.execute_reply.started":"2022-03-26T23:10:30.756696Z","shell.execute_reply":"2022-03-26T23:10:30.760080Z"},"trusted":true},"execution_count":51,"outputs":[]},{"cell_type":"markdown","source":"### Making the Neural Networks model with layers\n\nWe are going to use the **sequential neural networks** with a **LSTM** artificial recurrent neural network (RNN) architecture. The number of layers that we are going to have depends on our preference. I set up 100 layers as an experiment. The Dense function tells the machine that we want whatever input that we put into the model to give us that specific number of output. We can see the summary of our model after we set it up.","metadata":{}},{"cell_type":"code","source":"from keras.models import Sequential\nfrom keras.layers import Dense\nfrom keras.layers import LSTM\n\nmodel = Sequential()\nmodel.add(LSTM(100,input_shape=(input_n, features)))\nmodel.add(Dense(1))\nmodel.compile(loss='mean_squared_error', optimizer='adam')\n\nmodel.summary()\n","metadata":{"execution":{"iopub.status.busy":"2022-03-26T23:10:33.292523Z","iopub.execute_input":"2022-03-26T23:10:33.293056Z","iopub.status.idle":"2022-03-26T23:10:33.592226Z","shell.execute_reply.started":"2022-03-26T23:10:33.293017Z","shell.execute_reply":"2022-03-26T23:10:33.591263Z"},"trusted":true},"execution_count":52,"outputs":[]},{"cell_type":"markdown","source":"### Running the model multiple times\n\nNow that we have the model, we can run it multiple times, or epochs, to minimize the loss with each learning rate and gradient descent from the neural network.","metadata":{}},{"cell_type":"code","source":"model.fit(generator, epochs=100, batch_size=1)","metadata":{"execution":{"iopub.status.busy":"2022-03-26T23:10:36.155490Z","iopub.execute_input":"2022-03-26T23:10:36.155976Z","iopub.status.idle":"2022-03-26T23:14:46.032190Z","shell.execute_reply.started":"2022-03-26T23:10:36.155943Z","shell.execute_reply":"2022-03-26T23:14:46.031215Z"},"trusted":true},"execution_count":53,"outputs":[]},{"cell_type":"markdown","source":"### Loss History\n\nWhen we look at the loss history for neural network, we can see how many epochs we needed to run until the loss is actually minimized to a number that cannot be minimized further when running gradient descent for our neural network model.","metadata":{}},{"cell_type":"code","source":"PerEpochLoss=model.history.history['loss']\nplt.plot(range(len(PerEpochLoss)),PerEpochLoss)","metadata":{"execution":{"iopub.status.busy":"2022-03-26T23:14:46.034992Z","iopub.execute_input":"2022-03-26T23:14:46.035579Z","iopub.status.idle":"2022-03-26T23:14:46.635327Z","shell.execute_reply.started":"2022-03-26T23:14:46.035525Z","shell.execute_reply":"2022-03-26T23:14:46.634077Z"},"trusted":true},"execution_count":54,"outputs":[]},{"cell_type":"markdown","source":"As you can see, the loss reached a minimum after around 10-13 epochs. We can stop the gradient descent at around 13 epochs.\n\n### Fitting the Training set\n\nSo we wanted to test to see whether we can take the last n value of the training set to predict the first value of the test set. We are going to compare the predicted value with the actual value of the test set.","metadata":{}},{"cell_type":"code","source":"#taking the last 6 values of the training set and predict the first value of the test set\nlasttrainbatch=scaled_train[-6:]\nlasttrainbatch=lasttrainbatch.reshape((1,input_n, features))\n\nmodel.predict(lasttrainbatch)","metadata":{"execution":{"iopub.status.busy":"2022-03-26T23:14:46.638673Z","iopub.execute_input":"2022-03-26T23:14:46.640824Z","iopub.status.idle":"2022-03-26T23:14:47.481363Z","shell.execute_reply.started":"2022-03-26T23:14:46.640714Z","shell.execute_reply":"2022-03-26T23:14:47.480422Z"},"trusted":true},"execution_count":55,"outputs":[]},{"cell_type":"code","source":"scaled_test[0]\n#pretty close to the actual value!","metadata":{"execution":{"iopub.status.busy":"2022-03-26T23:14:47.483306Z","iopub.execute_input":"2022-03-26T23:14:47.483581Z","iopub.status.idle":"2022-03-26T23:14:47.490522Z","shell.execute_reply.started":"2022-03-26T23:14:47.483549Z","shell.execute_reply":"2022-03-26T23:14:47.489574Z"},"trusted":true},"execution_count":56,"outputs":[]},{"cell_type":"markdown","source":"As you can see, there is only a slight difference between the predicted value and the actual value. We will proceed to predict the 2021 value with our current neural network model now.","metadata":{}},{"cell_type":"code","source":"test_predictions=[]\n\nfirst_eval_batch= scaled_train[-input_n:]\ncurrent_batch= first_eval_batch.reshape((1,input_n,features))\n\nfor i in range(len(test)):\n    \n    currentprediction=model.predict(current_batch)[0]\n    test_predictions.append(currentprediction)\n    current_batch=np.append(current_batch[:,1:,:],[[currentprediction]],axis=1)","metadata":{"execution":{"iopub.status.busy":"2022-03-26T23:14:47.492243Z","iopub.execute_input":"2022-03-26T23:14:47.493212Z","iopub.status.idle":"2022-03-26T23:14:48.450058Z","shell.execute_reply.started":"2022-03-26T23:14:47.493120Z","shell.execute_reply":"2022-03-26T23:14:48.448135Z"},"trusted":true},"execution_count":57,"outputs":[]},{"cell_type":"markdown","source":"### Scaling back\n\nAfter we got our prediction figures, we should scale the figures back to actual wheat prices per ton instead of a figure between 0 and 1.","metadata":{}},{"cell_type":"code","source":"#transform this back to original scale\n\ntest_predictions= scaler.inverse_transform(test_predictions)\n\ntest['Predictions']= test_predictions","metadata":{"execution":{"iopub.status.busy":"2022-03-26T23:14:48.452230Z","iopub.execute_input":"2022-03-26T23:14:48.452668Z","iopub.status.idle":"2022-03-26T23:14:48.462136Z","shell.execute_reply.started":"2022-03-26T23:14:48.452622Z","shell.execute_reply":"2022-03-26T23:14:48.460461Z"},"trusted":true},"execution_count":58,"outputs":[]},{"cell_type":"markdown","source":"### Comparison of Prediction figures and Actual figures\n\nLet's compare the prediction value and the actual value of the wheat price.\n","metadata":{}},{"cell_type":"code","source":"test.plot()","metadata":{"execution":{"iopub.status.busy":"2022-03-26T23:14:48.464669Z","iopub.execute_input":"2022-03-26T23:14:48.465269Z","iopub.status.idle":"2022-03-26T23:14:49.159184Z","shell.execute_reply.started":"2022-03-26T23:14:48.465222Z","shell.execute_reply":"2022-03-26T23:14:49.157284Z"},"trusted":true},"execution_count":59,"outputs":[]},{"cell_type":"code","source":"from sklearn.metrics import mean_squared_error\n\nrmse=(mean_squared_error(test['Price_wheat_ton'],test['Predictions']))\nprint(rmse)","metadata":{"execution":{"iopub.status.busy":"2022-03-26T23:14:49.161554Z","iopub.execute_input":"2022-03-26T23:14:49.161977Z","iopub.status.idle":"2022-03-26T23:14:49.171514Z","shell.execute_reply.started":"2022-03-26T23:14:49.161931Z","shell.execute_reply":"2022-03-26T23:14:49.169783Z"},"trusted":true},"execution_count":60,"outputs":[]},{"cell_type":"markdown","source":"## What the Neural Network Model Tell Us\n\nAs seen in the model above, the neural network model can also predict the upward trend of the wheat price, but it doesn't have as good of a future predictive capability as compared to the Fourier feature polynomial regression model. From the RMSE we've calculated, it is similar to the RMSE we get from linear regression models. \n\nFrom what we can interpret, it is possible that models that have better modeling capability of the data (splines and multioutput models) might not have a good predictive capability due to overfitting. Overall, the fifth order Fourier feature polynomial regression model takes a moderate approach of infusing a proper modeling capability with a better predictive capability.","metadata":{}}]}